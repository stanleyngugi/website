<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SwahiliPro: A Framework for Linguistically-Aware Swahili Large Language Model Fine-tuning</title>
    <!-- Load JetBrains Mono font from Google Fonts -->
    <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;600;700&display=swap" rel="stylesheet">
    <!-- Load Tailwind CSS CDN -->
    <script src="https://cdn.tailwindcss.com"></script>
    <!-- Load KaTeX for LaTeX rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" xintegrity="sha384-wcIx8eH6J4z8388U/m8U8R9/bI7dJ6v/16oB/d+y2eD8p1i5p5b8y..." crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" xintegrity="sha384-F1qf5qF/e8gV/n8R3z3f5G..." crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" xintegrity="sha384-M0x9c35zG5z9x9g8y3R3z8H9y..." crossorigin="anonymous"></script>
    <style>
        /* Apply JetBrains Mono to the entire body */
        body {
            font-family: 'JetBrains Mono', monospace;
            background-color: #1b1d2e; /* Deep navy/charcoal background */
            color: #e8e8e8; /* Off-white gray for primary text */
            line-height: 1.65; /* Default line height for body text */
        }

        /* Custom utility for precise font sizes if Tailwind defaults aren't exact */
        .text-13px { font-size: 13px; }
        .text-15px { font-size: 15px; }
        .text-18px { font-size: 18px; }
        .text-22px { font-size: 22px; }

        /* Ensure links don't have default underlines or hover effects */
        a {
            text-decoration: none;
        }

        /* Adjust KaTeX to fit the minimalist tone */
        .katex {
            font-size: 1em; /* Ensure inline math doesn't look oversized */
        }
    </style>
</head>
<body class="bg-[#1b1d2e] text-[#e8e8e8]">
    <div class="max-w-3xl mx-auto p-8">
        <!-- Title (paper/article) -->
        <h1 class="text-18px font-bold text-[#ff5e3a] text-center mb-4">
            SwahiliPro: A Framework for Linguistically-Aware Swahili Large Language Model Fine-tuning
        </h1>

        <!-- Author + Meta (preprint, date) -->
        <p class="text-13px font-normal text-[#999999] leading-[1.2] text-center mb-12">
            An end-to-end pipeline to imbue Large Language Models with a deep, structural understanding of Swahili's rich morphology, pro-drop phenomena, and code-switching contexts.
        </p>

        <!-- Section Heading -->
        <h2 class="text-22px font-bold text-[#ff5e3a] border-b-2 border-[#ff5e3a] pb-2 mt-12 mb-6">
            1. Project Vision & Motivation
        </h2>
        <p class="mb-4 text-15px">
            Large Language Models (LLMs) have demonstrated remarkable capabilities, yet they often falter when applied to morphologically rich languages (MRLs) like Swahili. Standard subword tokenization algorithms, designed for English, tend to fracture Swahili's complex, agglutinative words into non-meaningful units, preventing the model from grasping the intricate grammatical relationships encoded within them. SwahiliPro is a proposed research and development framework designed to tackle this challenge head-on. The vision is not merely to build another Swahili model, but to engineer a principled, end-to-end pipeline that systematically teaches an LLM the fundamental linguistic structures of Swahili. This project moves beyond surface-level text processing to fine-tune a model that understands the why behind the words—how morphemes combine to create meaning, how context implies missing pronouns, and how to navigate the fluid boundary between Swahili and English in real-world usage.
        </p>

        <!-- Section Heading -->
        <h2 class="text-22px font-bold text-[#ff5e3a] border-b-2 border-[#ff5e3a] pb-2 mt-12 mb-6">
            2. The Core Problem Statement
        </h2>
        <p class="mb-4 text-15px">
            The framework is architected to address three specific, high-impact challenges in Swahili NLP. The first is <span class="font-bold">Morphological Blindness</span>, where Swahili words can contain a root, multiple prefixes, and suffixes that denote tense, aspect, subject, and object. For example, <span class="font-bold">atanipenda</span> ("he/she will love me") is a single word containing four distinct morphemes (<span class="font-bold">a-ta-ni-penda</span>). Standard tokenizers might split this as <span class="font-bold">at, ani, penda</span>, losing the grammatical information. The second challenge is <span class="font-bold">Contextual Ambiguity (Pro-Drop)</span>. Swahili is a pro-drop language where subject pronouns are often omitted because the subject is marked on the verb itself (e.g., <span class="font-bold">ninakula</span> for "I am eating"). A standard LLM struggles to infer the dropped subject, a critical task for true comprehension and coherent generation. The third issue is the <span class="font-bold">Code-Switching (CS) Reality</span>. In modern digital and spoken Swahili, code-switching with English is ubiquitous. An effective model must not only recognize both languages but also develop aligned representations, understanding that <span class="font-bold">gari</span> and <span class="font-bold">car</span> can be semantically equivalent in the same context. Finally, an overarching challenge is the <span class="font-bold">NLP Tooling Scarcity</span>, as unlike high-resource languages, there is no single, comprehensive, state-of-the-art toolkit for Swahili linguistic analysis. A practical solution requires a pragmatic, composite approach, intelligently combining the best available tools.
        </p>

        <!-- Section Heading -->
        <h2 class="text-22px font-bold text-[#ff5e3a] border-b-2 border-[#ff5e3a] pb-2 mt-12 mb-6">
            3. The Proposed Solution: SwahiliMaalumLM Architectural Deep Dive
        </h2>
        <p class="mb-4 text-15px">
            The proposed solution is a modular, three-phase pipeline that transforms raw data into a linguistically-aware, fine-tuned LLM.
        </p>
        <h3 class="text-18px font-bold text-[#ff5e3a] mt-8 mb-4">
            Phase 1: The Linguistic Foundation Pipeline (Data Preparation)
        </h3>
        <p class="mb-4 text-15px">
            This is the heart of the framework's innovation, where raw text is enriched with explicit linguistic structure. The process begins with <span class="font-bold">Stage 1: Raw Processing & Intelligent Task Flagging</span>. Here, raw text is cleaned and subjected to a two-tier flagging system. A fast regex-based check first identifies potential candidates for pro-drop or code-switching. These candidates are then passed to a more sophisticated SwahiliAnnotator for detailed analysis (e.g., POS-based confirmation), optimizing processing time. The output is a clean, intermediate dataset where each entry is flagged with the specialized tasks it is suitable for (e.g., ["general_lm", "code_switch_candidate"]).
        </p>
        <p class="mb-4 text-15px">
            Next is <span class="font-bold">Stage 2: Feature Generation & Critical Index Mapping (Novelty Core)</span>. A key principle is <span class="font-bold">Tokenizer Consistency</span>, where a base Hugging Face tokenizer is loaded once, its special tokens are configured (e.g., adding <span class="font-bold"><sep></span>), and this exact instance is saved. This single, consistent tokenizer instance becomes the source of truth for all downstream processes, eliminating a common source of error. <span class="font-bold">Task-Specific Data Synthesis</span> is then performed. For <span class="font-bold">Morphology</span>, the SwahiliAnnotator uses a segmentation model (e.g., a custom-trained Morfessor) to insert morpheme boundaries with a special <span class="font-bold"><sep></span> token (<span class="font-bold">a<sep>ta<sep>ni<sep>penda</span>). This makes the sub-word structure explicit to the LLM. For <span class="font-bold">Pronoun-Drop (Experimental)</span>, heuristics identify a pronoun and its associated verb. A new sentence is created with the pronoun removed, and this new sentence is re-annotated to reliably find the character offsets of the verb in its new position. The character offsets are then mapped to the final token indices in the fully tokenized sequence. The output is a JSONL entry containing the morpheme-marked sentence without the pronoun, along with the token indices of the key verb (<span class="font-bold">verb_token_indices_mapped</span>) and a label for the dropped pronoun (<span class="font-bold">pronoun_label_id</span>). For <span class="font-bold">Code-Switching</span>, a Swahili word is replaced with its English equivalent. Two parallel sentences are created: the morpheme-marked CS sentence (<span class="font-bold">cs_sentence_marked</span>) and the morpheme-marked original Swahili sentence (<span class="font-bold">sw_equivalent_marked</span>). The character spans for the English term (in the CS sentence) and the original Swahili term (in the Swahili sentence) are found and mapped to their respective token indices. The output is a JSONL entry with both sentences and their corresponding token index spans (<span class="font-bold">cs_span_token_indices_mapped</span> and <span class="font-bold">sw_span_token_indices_mapped</span>).
        </p>

        <h3 class="text-18px font-bold text-[#ff5e3a] mt-8 mb-4">
            Phase 2: Phased Adaptation & Custom Learning (LLM Fine-tuning)
        </h3>
        <p class="mb-4 text-15px">
            This phase uses the richly annotated data to fine-tune an LLM with surgical precision using PEFT (QLoRA). The <span class="font-bold">Model Architecture</span> starts with a base LLM augmented with LoRA adapters and a small, trainable <span class="font-bold">ContextualSubjectHead</span>—a classification layer designed specifically for the pro-drop task. A <span class="font-bold">Phased Training Strategy (Novelty Core)</span> is employed. In <span class="font-bold">Phase 1 (Morphology Focus)</span>, the model is first trained only on the standard Causal LM objective using the morpheme-marked (<span class="font-bold"><sep></span>) text. The goal is to first teach the model the "new vocabulary" and structure of segmented Swahili. Only a <span class="font-bold">morph_adapter</span> is trained; the task-specific adapter and head are frozen. In <span class="font-bold">Phase 2 (Task Focus)</span>, the <span class="font-bold">morph_adapter</span> is frozen but kept active. A second <span class="font-bold">task_adapter</span> and the ContextualSubjectHead are unfrozen. The model is now trained on a combined loss function. This <span class="font-bold">Dual Custom Loss Function (Novelty Core)</span> includes a <span class="font-bold">Base Causal LM Loss</span>, which is the standard "predict the next token" loss, now benefiting from the explicit morphological markers. It also includes a <span class="font-bold">Subject Prediction Loss</span>, where the embeddings at the <span class="font-bold">verb_token_indices_mapped</span> are pooled and fed into the ContextualSubjectHead. A cross-entropy loss is calculated against the <span class="font-bold">pronoun_label_id</span>, explicitly teaching the model to predict the dropped subject from the verb's context. Finally, a <span class="font-bold">Code-Switch Alignment Loss</span> uses the <span class="font-bold">cs_span_token_indices_mapped</span> and <span class="font-bold">sw_span_token_indices_mapped</span> to pool the embeddings for the English term and its Swahili equivalent (requiring an auxiliary forward pass on the Swahili text). A cosine similarity loss then "pulls" these two representations closer together in the model's embedding space, teaching it semantic equivalence.
        </p>

        <h3 class="text-18px font-bold text-[#ff5e3a] mt-8 mb-4">
            Phase 3: Evaluation & Inference
        </h3>
        <p class="mb-4 text-15px">
            The final phase involves <span class="font-bold">Evaluation</span>, where the model is evaluated on both standard metrics (Perplexity) and task-specific metrics (Subject Prediction Accuracy, CS Embedding Similarity). This provides a holistic view of its acquired linguistic capabilities. For <span class="font-bold">Inference</span>, the final model can be used for generation, with input prompts being passed through the same SwahiliAnnotator to ensure consistency with the training data.
        </p>

        <!-- Section Heading -->
        <h2 class="text-22px font-bold text-[#ff5e3a] border-b-2 border-[#ff5e3a] pb-2 mt-12 mb-6">
            4. Innovations & Novel Contributions
        </h2>
        <p class="mb-4 text-15px">
            This framework's novelty lies in its systematic, linguistically-informed approach. The <span class="font-bold">Composite Linguistic Annotator</span> is a pragmatic SwahiliAnnotator that combines multiple off-the-shelf tools (Morfessor, Polyglot, HF POS Taggers, UniMorph) to create a robust annotation pipeline, addressing the real-world scarcity of a single, perfect tool for Swahili. The <span class="font-bold">Systematic Data Representation for Linguistics</span> is another key innovation—the creation of structured JSONL files with pre-calculated, validated character-to-token index mappings for specific linguistic phenomena. This is a robust data engineering approach crucial for complex training regimes. The <span class="font-bold">Phased PEFT Training Strategy</span> is a novel training methodology that stabilizes learning on structurally modified text before introducing more complex, multi-objective tasks. The <span class="font-bold">Multi-Objective Custom Loss Function</span> is a unique combination of a standard Causal LM loss with a classification loss (for pro-drop) and a contrastive/similarity loss (for code-switching) within a single training loop, allowing the model to learn diverse linguistic skills simultaneously. Finally, the <span class="font-bold">End-to-End Consistency</span> achieved by the rigorous enforcement of a single, saved tokenizer and consistent configuration flow from data preparation through to inference represents a mature and robust MLOps practice, critical for reproducibility and reliability.
        </p>

        <!-- Section Heading -->
        <h2 class="text-22px font-bold text-[#ff5e3a] border-b-2 border-[#ff5e3a] pb-2 mt-12 mb-6">
            5. Technical Stack
        </h2>
        <ul class="list-disc list-inside mb-4 text-15px">
            <li><span class="font-bold">Language:</span> Python</li>
            <li><span class="font-bold">Core Libraries:</span> PyTorch, Hugging Face (Transformers, PEFT, Accelerate, Tokenizers)</li>
            <li><span class="font-bold">NLP Tools:</span> Morfessor, Polyglot, UniMorph-Inflect</li>
            <li><span class="font-bold">Data Handling:</span> Pandas, PyYAML</li>
            <li><span class="font-bold">Infrastructure:</span> High-Performance Computing (HPC) cluster with GPUs (A100/H100)</li>
        </ul>

        <!-- Section Heading -->
        <h2 class="text-22px font-bold text-[#ff5e3a] border-b-2 border-[#ff5e3a] pb-2 mt-12 mb-6">
            6. Project Status & Future Directions
        </h2>
        <p class="mb-4 text-15px">
            <span class="font-bold">Status:</span> Proposed Research & Development. The framework is fully designed, and the component logic is specified.
        </p>
        <p class="mb-4 text-15px">
            <span class="font-bold">Next Steps:</span>
        </p>
        <ul class="list-disc list-inside mb-4 text-15px">
            <li><span class="font-bold">Implementation:</span> Build out the data preparation and training scripts based on the detailed architecture.</li>
            <li><span class="font-bold">Baseline Experiments:</span> Run initial experiments focusing on the impact of morphological segmentation alone.</li>
            <li><span class="font-bold">Full Model Training:</span> Execute the full phased training with the dual custom losses on a comprehensive Swahili corpus (e.g., WURA).</li>
        </ul>
    </div>
</body>
</html>
